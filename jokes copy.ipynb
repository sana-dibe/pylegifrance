{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ha\n",
    "jokes = ha.joke_datasets['reddit_jokes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, config2py\n",
    "os.environ[\"OPENAI_API_KEY\"] = config2py.config_getter(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from lkj import chunker\n",
    "import oa\n",
    "\n",
    "def reddit_joke_metadata_to_key_and_text(metdata: dict, *, sep='\\n'):\n",
    "    return f\"{metdata['title']}{sep}({metdata['body']})\"\n",
    "\n",
    "# segment = next(chunker(jokes, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jokes_list = list(chunker(jokes, 20))\n",
    "# jokes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = list(map(reddit_joke_metadata_to_key_and_text, segment))\n",
    "# cumul2 = oa.embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from typing import List\n",
    "def tokenize(text: str, encoding_name : str ) -> List[str]:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    int_tokens = encoding.encode(text)\n",
    "    str_tokens = [encoding.decode_single_token_bytes(token) for token in int_tokens]\n",
    "    return str_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Sana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from typing import List, Tuple\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stop_words(tokens: List[str]) -> List[str]:\n",
    "#     return [token for token in tokens if token.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_name = \"cl100k_base\" \n",
    "\n",
    "def process_joke(joke: dict, max_tokens: int) -> Tuple[str, dict, bool]:\n",
    "    joke_text = reddit_joke_metadata_to_key_and_text(joke)\n",
    "    # joke_tokens = tokenize(joke_text)\n",
    "    # joke_tokens = remove_stop_words(joke_tokens)\n",
    "    joke_tokens = num_tokens_from_string(joke_text, encoding_name)\n",
    "\n",
    "    if joke_tokens > max_tokens:\n",
    "        # joke_tokens = joke_tokens[:max_tokens]\n",
    "        return \"\", {}, True  # Skips the joke\n",
    "\n",
    "    # truncated_text = ' '.join(joke_tokens)\n",
    "    # return truncated_text\n",
    "    metadata = {\n",
    "        \"source\": \"Reddit\",\n",
    "        \"id\": joke['id'],\n",
    "        \"score\": joke['score']\n",
    "    }\n",
    "    return joke_text, metadata, False\n",
    "\n",
    "# def process_jokes(jokes: List[dict], max_tokens: int) -> List[Tuple[str, dict]]:\n",
    "#     processed_jokes = map(lambda joke: process_joke(joke, max_tokens), jokes)\n",
    "#     return [(joke_text, metadata)  for joke_text, metadata, skip in processed_jokes if not skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps required pour un batch de 20: 0.7049269676208496 secondes\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 8192\n",
    "batch_size =  20\n",
    "single_batch = next(chunker(jokes, batch_size))\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "texts = [joke_text for joke_text, _, skip in map(lambda joke: process_joke(joke, max_tokens), single_batch) if not skip]\n",
    "embeddings = oa.embeddings(texts)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Temps required pour un batch de 20:\", elapsed_time, \"secondes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps total pour tout le dataset: 6857.529541015625 secondes\n"
     ]
    }
   ],
   "source": [
    "## If we need to include all jokes (no exceed of the maximum token limit (max_tokens) under the best conditions)\n",
    "total_batches = (len(jokes) + batch_size - 1) // batch_size  \n",
    "total_time = total_batches * elapsed_time\n",
    "print(\"Temps total pour tout le dataset:\", total_time, \"secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hours_minutes_seconds(seconds):\n",
    "    hours = seconds // 3600 \n",
    "    remaining_seconds = seconds % 3600  \n",
    "    minutes = remaining_seconds // 60  \n",
    "    remaining_seconds = remaining_seconds % 60  \n",
    "    return hours, minutes, remaining_seconds\n",
    "\n",
    "hours, minutes, remaining_seconds = seconds_to_hours_minutes_seconds(total_time)\n",
    "print(f\"{total_time} secondes ==> {hours} heures, {minutes} minutes, {remaining_seconds} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=rag_jokes)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chroma run --path /Users/Sana/chromadb   \n",
    "\n",
    "import chromadb\n",
    "\n",
    "## Use open ai embedding in chroma:\n",
    "\n",
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "#                 api_key= os.environ[\"OPENAI_API_KEY\"] ,\n",
    "#                 model_name=\"text-embedding-3-small\"\n",
    "#             )\n",
    "\n",
    "# chroma_client = chromadb.Client()\n",
    "# chroma_client = chromadb.PersistentClient(path=\"/Users/Sana/chromadb\")\n",
    "# chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "from chromadb.config import Settings\n",
    "chroma_client = chromadb.HttpClient(\n",
    "    settings=Settings(chroma_client_auth_provider=\"chromadb.auth.token.TokenAuthClientProvider\",\n",
    "                      chroma_client_auth_credentials=\"test-token\"))\n",
    "chroma_client.heartbeat()  \n",
    "# chroma_client.get_version()  \n",
    "# chroma_client.list_collections()  \n",
    "# chroma_client.delete_collection(name=\"jokes_rag\")\n",
    "# chroma_client.delete_collection(name=\"reddit_jokes\")\n",
    "\n",
    "# joke_collection = chroma_client.get_or_create_collection(name=\"rag_jokes\", embedding_function=openai_ef)\n",
    "joke_collection = chroma_client.get_or_create_collection(name=\"rag_jokes\")\n",
    "chroma_client.list_collections() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 8192\n",
    "batch_size = 20\n",
    "joke_chunker = chunker(jokes, batch_size)  # Generator for batching jokes\n",
    "\n",
    "for single_batch in joke_chunker:\n",
    "    # Process each joke in the batch and filter out the ones that exceed max_tokens\n",
    "    processed_batch = map(lambda joke: process_joke(joke, max_tokens), single_batch)\n",
    "    valid_jokes = [(joke_text, metadata) for joke_text, metadata, skip in processed_batch if not skip]\n",
    "\n",
    "    # Separate the joke texts, metadata, and IDs for embedding calculation and storage\n",
    "    if valid_jokes:\n",
    "        texts, metadatas = zip(*valid_jokes)\n",
    "        ids = [metadata['id'] for metadata in metadatas]\n",
    "\n",
    "        # Compute embeddings for the filtered jokes\n",
    "        embeddings = oa.embeddings(list(texts))\n",
    "\n",
    "        # Store embeddings, texts, IDs, and metadata in the Chroma database\n",
    "        for embedding, joke_text, joke_id, metadata in zip(embeddings, texts, ids, metadatas):\n",
    "            joke_collection.add(embeddings=embedding, documents=joke_text, ids=joke_id, metadatas=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jokes = joke_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_jokes['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embed query with the same model\n",
    "query = \"What is the funiest joke?\"\n",
    "embed_query = oa.embeddings(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then, we'll retrieve the top-k most relevant chunks by extracting the closest embedded chunks to our embedded query. We use cosine distance\n",
    "# joke_collection.modify(metadata={\"hnsw:space\": \"cosine\"})\n",
    "results = joke_collection.query(\n",
    "    query_embeddings=[embed_query],\n",
    "    n_results=10,\n",
    "    #include = [ \"documents\" ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the Funniest Joke You Can Think of\\n(Make it hilarious please.)\\nWhats the funniest joke you know?\\n(You.)\\nWorld\\'s Funniest Joke\\n(The \"world\\'s funniest joke\" is a term used by Richard Wiseman of the University of Hertfordshire in 2002 to summarize one of the results of his research. For his experiment, named LaughLab, he created a website where people could rate and submit jokes. Purposes of the research included discovering the joke that had the widest appeal and understanding among different cultures, demographics and countries.\\n\\nThe History Channel eventually hosted a special on the subject.\\n\\n\\nThe winning joke, which was later found to be based on a 1951 Goon Show sketch by Spike Milligan,was submitted by Gurpal Gosal of Manchester:\\n\\n\\n   *Two hunters are out in the woods when one of them collapses. He doesn\\'t seem to be breathing and his eyes are glazed. The other guy whips out his phone and calls the emergency services. He gasps, \"My friend is dead! What can I do?\" The operator says \"Calm down. I can help. First, let\\'s make sure he\\'s dead.\" There is a silence, then a gun shot is heard. Back on the phone, the guy says \"OK, now what?\"*)\\nWhat is the best joke you have heard?\\n(Any best joke)\\nWhat is one of the funniest simple joke you have ever heard?\\n(Literally... made you die laughing\\n\\ne.g. Why did the monkey fall of the tree? \\nbc it died\\n\\ne.g. How do you keep an idiot in suspense?)\\nWorld\\'s funniest joke\\n(The \"world\\'s funniest joke\" is a term used by Richard Wiseman of the University of Hertfordshire in 2002 to summarize one of the results of his research. For his experiment, named LaughLab, he created a website where people could rate and submit jokes.)\\nWhat\\'s the best anti-joke ever?\\n(This one.)\\nI\\'m looking for the funniest short joke ever\\n(Not short as in small, short as in short)\\nThe funniest joke ever...\\n(This one:)\\nWhat kind of joke is the best joke?\\n(The one shoved up your ass.\\n\\n(As told by my adorable and naughty 7 year old).)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = \"\\n\".join(str(item) for item in results['documents'][0])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(invite, instance_client, modele=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": invite}]\n",
    "    reponse = instance_client.chat.completions.create(\n",
    "        model=modele,\n",
    "        messages=messages,\n",
    "        max_tokens=50,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return reponse.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're looking for the world's funniest joke, which was determined by Richard Wiseman's LaughLab experiment. The winning joke, submitted by Gurpal Gosal of Manchester, goes like this:\n",
      "\n",
      "\"Two hunters are out in the woods when one of them collapses. He doesn't seem to be breathing and his eyes are glazed. The other guy whips out his phone and calls the emergency services. He gasps, 'My friend is dead! What can I do?' The operator says 'Calm down. I can help. First, let's make sure he's dead.' There is a silence, then a gun shot is heard. Back on the phone, the guy says 'OK, now what?'\"\n",
      "\n",
      "It's always interesting to see what jokes people find the funniest!\n"
     ]
    }
   ],
   "source": [
    "## Response generation\n",
    "\n",
    "prompt=f'```{res}```is the funiest joke'\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about reddit jokes.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "response_message = response.choices[0].message.content\n",
    "\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Response generation\n",
    "\n",
    "context = [item for item in results['documents']]\n",
    "sources = [item[\"source\"] for sublist in results['metadatas'] for item in sublist]\n",
    "scores = [item[\"score\"] for sublist in results['metadatas'] for item in sublist]\n",
    "user_content = f\"query: {query}, context: {context}\"\n",
    "messages = [{\"role\": role, \"content\": content} for role, content in [\n",
    "        (\"system\",  \"Answer the query using the context provided. Be succinct.\"), \n",
    "        (\"user\", user_content)] if content]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "response_message = response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "\"question\": query,\n",
    "# \"sources\": sources,\n",
    "\"score\" : scores,\n",
    "\"answer\": response_message\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the funiest joke?',\n",
       " 'score': [0, 0, 828, 0, 0, 0, 0, 11, 0, 0],\n",
       " 'answer': 'The funniest joke, according to research by Richard Wiseman, is: \"Two hunters are out in the woods when one of them collapses. He doesn\\'t seem to be breathing and his eyes are glazed. The other guy whips out his phone and calls the emergency services. He gasps, \\'My friend is dead! What can I do?\\' The operator says \\'Calm down. I can help. First, let\\'s make sure he\\'s dead.\\' There is a silence, then a gun shot is heard. Back on the phone, the guy says \\'OK, now what?\\'\"'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What is the Funniest Joke You Can Think of\\n(Make it hilarious please.)',\n",
       "  'Whats the funniest joke you know?\\n(You.)',\n",
       "  'World\\'s Funniest Joke\\n(The \"world\\'s funniest joke\" is a term used by Richard Wiseman of the University of Hertfordshire in 2002 to summarize one of the results of his research. For his experiment, named LaughLab, he created a website where people could rate and submit jokes. Purposes of the research included discovering the joke that had the widest appeal and understanding among different cultures, demographics and countries.\\n\\nThe History Channel eventually hosted a special on the subject.\\n\\n\\nThe winning joke, which was later found to be based on a 1951 Goon Show sketch by Spike Milligan,was submitted by Gurpal Gosal of Manchester:\\n\\n\\n   *Two hunters are out in the woods when one of them collapses. He doesn\\'t seem to be breathing and his eyes are glazed. The other guy whips out his phone and calls the emergency services. He gasps, \"My friend is dead! What can I do?\" The operator says \"Calm down. I can help. First, let\\'s make sure he\\'s dead.\" There is a silence, then a gun shot is heard. Back on the phone, the guy says \"OK, now what?\"*)',\n",
       "  'What is the best joke you have heard?\\n(Any best joke)',\n",
       "  'What is one of the funniest simple joke you have ever heard?\\n(Literally... made you die laughing\\n\\ne.g. Why did the monkey fall of the tree? \\nbc it died\\n\\ne.g. How do you keep an idiot in suspense?)',\n",
       "  'World\\'s funniest joke\\n(The \"world\\'s funniest joke\" is a term used by Richard Wiseman of the University of Hertfordshire in 2002 to summarize one of the results of his research. For his experiment, named LaughLab, he created a website where people could rate and submit jokes.)',\n",
       "  \"What's the best anti-joke ever?\\n(This one.)\",\n",
       "  \"I'm looking for the funniest short joke ever\\n(Not short as in small, short as in short)\",\n",
       "  'The funniest joke ever...\\n(This one:)',\n",
       "  'What kind of joke is the best joke?\\n(The one shoved up your ass.\\n\\n(As told by my adorable and naughty 7 year old).)']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blagues trouvée :\n",
      "{'body': 'Methed Up', 'id': '4s7b2g', 'score': 828, 'title': 'What do you call Mike Tyson on drugs?'}\n",
      "{'body': 'Apparently it just changes the color of the baby', 'id': '49vgvc', 'score': 828, 'title': \"After my vasectomy I thought I couldn't get my wife pregenant\"}\n",
      "{'body': 'The \"world\\'s funniest joke\" is a term used by Richard Wiseman of the University of Hertfordshire in 2002 to summarize one of the results of his research. For his experiment, named LaughLab, he created a website where people could rate and submit jokes. Purposes of the research included discovering the joke that had the widest appeal and understanding among different cultures, demographics and countries.\\n\\nThe History Channel eventually hosted a special on the subject.\\n\\n\\nThe winning joke, which was later found to be based on a 1951 Goon Show sketch by Spike Milligan,was submitted by Gurpal Gosal of Manchester:\\n\\n\\n   *Two hunters are out in the woods when one of them collapses. He doesn\\'t seem to be breathing and his eyes are glazed. The other guy whips out his phone and calls the emergency services. He gasps, \"My friend is dead! What can I do?\" The operator says \"Calm down. I can help. First, let\\'s make sure he\\'s dead.\" There is a silence, then a gun shot is heard. Back on the phone, the guy says \"OK, now what?\"*', 'id': '1tr9ga', 'score': 828, 'title': \"World's Funniest Joke\"}\n"
     ]
    }
   ],
   "source": [
    "joke_with_score_828 = []\n",
    "for joke in jokes: \n",
    "    if joke[\"score\"] == 828:\n",
    "        joke_with_score_828.append(joke)\n",
    "\n",
    "if joke_with_score_828:\n",
    "    print(\"Blagues trouvée :\")\n",
    "    for joke in joke_with_score_828:\n",
    "        print(joke)\n",
    "else:\n",
    "    print(\"Aucune blague trouvée avec un score de 828.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
