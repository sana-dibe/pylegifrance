{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Légifrance data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I will use pylegifrance library : https://github.com/rdassignies/pylegifrance/?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, config2py\n",
    "os.environ[\"LEGIFRANCE_CLIENT_ID\"] = config2py.config_getter(\"LEGIFRANCE_CLIENT_ID\")\n",
    "os.environ[\"LEGIFRANCE_CLIENT_SECRET\"] = config2py.config_getter(\"LEGIFRANCE_CLIENT_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:35:18,794 - root - INFO - Connexion à l'api legifrance réussie.\n"
     ]
    }
   ],
   "source": [
    "from pylegifrance import LegiHandler\n",
    "client = LegiHandler()\n",
    "client.set_api_keys(legifrance_api_key=config2py.config_getter(\"LEGIFRANCE_CLIENT_ID\"), legifrance_api_secret=config2py.config_getter(\"LEGIFRANCE_CLIENT_SECRET\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:35:49,649 - pylegifrance.process.processors - INFO - Nombre de résultats trouvés: 1\n"
     ]
    }
   ],
   "source": [
    "from pylegifrance import recherche_CODE\n",
    "\n",
    "doc_integrale_travail = recherche_CODE(code_name=\"Code du travail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_integrale_travail[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['executionTime', 'dereferenced', 'id', 'idConteneur', 'cid', 'title', 'nor', 'eli', 'alias', 'jorfText', 'jurisState', 'visa', 'modifDate', 'jurisDate', 'dateDebutVersion', 'dateFinVersion', 'signers', 'prepWork', 'dateParution', 'dateTexte', 'numParution', 'notice', 'nota', 'inap', 'textNumber', 'textAbroge', 'etat', 'dossiersLegislatifs', 'nature', 'resume', 'rectificatif', 'motsCles', 'appellations', 'liens', 'observations', 'sections', 'articles', 'pagePdf', 'fileName', 'fileSize', 'filePath'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_integrale_travail[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Voir le contenu de doc_integrale\n",
    "import json\n",
    "\n",
    "with open('code_travail.json', 'w', encoding='utf-8') as fichier:\n",
    "    json.dump(doc_integrale_travail, fichier, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_infos(element, infos=[]):\n",
    "    if isinstance(element, dict):\n",
    "        if 'pathTitle' in element and 'content' in element:\n",
    "            infos.append({\n",
    "                'pathTitle': element['pathTitle'],\n",
    "                'content': element['content']\n",
    "            })\n",
    "\n",
    "        for key, value in element.items():\n",
    "            extraire_infos(value, infos)\n",
    "\n",
    "    elif isinstance(element, list):\n",
    "        for item in element:\n",
    "            extraire_infos(item, infos)\n",
    "\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_extraites_code_travail = extraire_infos(doc_integrale_travail[0]['sections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11520"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(infos_extraites_code_travail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in infos_extraites_code_travail:\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:36:30,611 - pylegifrance.process.processors - INFO - Nombre de résultats trouvés: 1\n"
     ]
    }
   ],
   "source": [
    "## pour sos oxygène, c'est que les données de santé, sécurité sociale, ....\n",
    "code_sante = recherche_CODE(code_name=\"Code de la santé publique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_extraites_code_sante = extraire_infos(code_sante[0]['sections'])\n",
    "infos_extraites_code_sante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25819"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(infos_extraites_code_sante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('code_sante.json', 'w', encoding='utf-8') as fichier:\n",
    "    json.dump(infos_extraites_code_sante, fichier, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict with content in text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pathTitle', 'content'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos_extraites_code_sante[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The content is in html format, so I convert it to text using utf8 encoding and I ignore links as they have not a meaning\n",
    "\n",
    "import html2text\n",
    "\n",
    "def convert_content(content, ig_links=True):\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = ig_links\n",
    "    if isinstance(content, bytes):\n",
    "        content = content.decode('utf-8')\n",
    "    return h.handle(content)\n",
    "\n",
    "converted_infos = []\n",
    "\n",
    "for item in infos_extraites_code_sante:\n",
    "    converted_item = {\n",
    "        'pathTitle': item['pathTitle'],\n",
    "        'content': convert_content(item['content'])\n",
    "    }\n",
    "    converted_infos.append(converted_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25819"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from lkj import chunker\n",
    "import oa\n",
    "\n",
    "\n",
    "def pathTitle_list_to_text(metadata: list, sep='\\n'):\n",
    "    return sep.join(metadata['pathTitle'])\n",
    "\n",
    "def pathContent_metadata_to_text(metadata: dict, *, sep='\\n'):\n",
    "    return f\"{sep.join(metadata['pathTitle'])}{sep}{metadata['content']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import tiktoken\n",
    "\n",
    "def tokenize(text: str, encoding_name : str ) -> List[str]:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    int_tokens = encoding.encode(text)\n",
    "    str_tokens = [encoding.decode_single_token_bytes(token) for token in int_tokens]\n",
    "    return str_tokens\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lkj import chunker\n",
    "encoding_name = \"cl100k_base\" \n",
    "\n",
    "def process_data(data: dict, max_tokens: int) -> Tuple[str, dict, bool]:\n",
    "    code_text = pathTitle_metadata_to_text(data)\n",
    "    # joke_tokens = tokenize(joke_text)\n",
    "    # joke_tokens = remove_stop_words(joke_tokens)\n",
    "    joke_tokens = num_tokens_from_string(joke_text, encoding_name)\n",
    "\n",
    "    if joke_tokens > max_tokens:\n",
    "        # joke_tokens = joke_tokens[:max_tokens]\n",
    "        return \"\", {}, True  # Skips the joke\n",
    "\n",
    "    # truncated_text = ' '.join(joke_tokens)\n",
    "    # return truncated_text\n",
    "    metadata = {\n",
    "        \"source\": \"Reddit\",\n",
    "        \"id\": joke['id'],\n",
    "        \"score\": joke['score']\n",
    "    }\n",
    "    return joke_text, metadata, False\n",
    "\n",
    "# def process_jokes(jokes: List[dict], max_tokens: int) -> List[Tuple[str, dict]]:\n",
    "#     processed_jokes = map(lambda joke: process_joke(joke, max_tokens), jokes)\n",
    "#     return [(joke_text, metadata)  for joke_text, metadata, skip in processed_jokes if not skip]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
